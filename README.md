# Data Engineering with Python

## Chapter 2. Building Infrastructure

### Make build (docker build python image & docker compose up)

ì±…ì—ì„œëŠ” Airflow, NiFi, PostgreSQL, Elasticsearch, Kibana, Kafka, Spark ë“± ëª¨ì¡°ë¦¬ ë‹¤ ë¡œì»¬ í™˜ê²½ì—ì„œ ì„¤ì¹˜í•´ì„œ ì‹¤ìŠµí•œë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ë‚´ê°€ ì•„ì£¼ ì‹«ì–´í•˜ëŠ” ìƒí™©ì´ë¯€ë¡œ ë‹¹ì—°í•˜ê²Œ Dockerë¥¼ í™œìš©í•´ì„œ í™˜ê²½ì„ êµ¬ì¶•í–ˆë‹¤.

#### ë²„ì „ ê´€ë¦¬

|Software / hardware|OS requirements|
|:------:|:---:|
|Python|3.12.8|
|Nifi|apache/nifi:1.28.0|
|PostgreSQL|postgres:13|
|ElasticSearch|elasticsearch:7.17.28|
|Kibana|kibana:7.17.28|

```bash
app-py3.12 âœ˜ {seilylook} ğŸ€ make build
==============================================
Exporting Python dependencies to requirements.txt...
==============================================
poetry export -f requirements.txt --output requirements.txt --without-hashes --with dev
Warning: poetry-plugin-export will not be installed by default in a future version of Poetry.
In order to avoid a breaking change and make your automation forward-compatible, please install poetry-plugin-export explicitly. See https://python-poetry.org/docs/plugins/#using-plugins for details on how to install a plugin.
To disable this warning run 'poetry config warnings.export false'.


==============================================
Building Docker image python-app:latest...
==============================================
docker build -t python-app:latest .
[+] Building 1.7s (20/20) FINISHED                                                                                               docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                             0.0s
 => => transferring dockerfile: 1.14kB                                                                                                           0.0s
 => [internal] load metadata for docker.io/library/python:3.12-slim                                                                              1.5s
 => [auth] library/python:pull token for registry-1.docker.io                                                                                    0.0s
 => [internal] load .dockerignore                                                                                                                0.0s
 => => transferring context: 2B                                                                                                                  0.0s
 => [internal] load build context                                                                                                                0.0s
 => => transferring context: 33.38kB                                                                                                             0.0s
 => [builder 1/5] FROM docker.io/library/python:3.12-slim@sha256:aaa3f8cb64dd64e5f8cb6e58346bdcfa410a108324b0f28f1a7cc5964355b211                0.0s
 => CACHED [stage-1  2/10] RUN apt-get update &&     apt-get install -y --no-install-recommends     default-jdk     procps     wget     libpq5   0.0s
 => CACHED [stage-1  3/10] WORKDIR /app                                                                                                          0.0s
 => CACHED [builder 2/5] WORKDIR /app                                                                                                            0.0s
 => CACHED [builder 3/5] RUN apt-get update &&     apt-get install -y --no-install-recommends     build-essential     libpq-dev     python3-dev  0.0s
 => CACHED [builder 4/5] COPY requirements.txt .                                                                                                 0.0s
 => CACHED [builder 5/5] RUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt                                      0.0s
 => CACHED [stage-1  4/10] COPY --from=builder /app/wheels /wheels                                                                               0.0s
 => CACHED [stage-1  5/10] COPY --from=builder /app/requirements.txt .                                                                           0.0s
 => CACHED [stage-1  6/10] RUN pip install --no-cache /wheels/*                                                                                  0.0s
 => [stage-1  7/10] COPY src/ src/                                                                                                               0.0s
 => [stage-1  8/10] COPY tests/ tests/                                                                                                           0.0s
 => [stage-1  9/10] COPY data/ data/                                                                                                             0.0s
 => [stage-1 10/10] COPY conf/ conf/                                                                                                             0.0s
 => exporting to image                                                                                                                           0.0s
 => => exporting layers                                                                                                                          0.0s
 => => writing image sha256:a7003da4b1bb9a04c9d98b7a386e9c28c84efe83df2ae20e99abc16cedf9e3fb                                                     0.0s
 => => naming to docker.io/library/python-app:latest                                                                                             0.0s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/4go6dhhdb6ahlu1m1xmndgsou

What's next:
    View a summary of image vulnerabilities and recommendations â†’ docker scout quickview 


==============================================
Constructing Docker Containers...
==============================================
docker compose up -d
WARN[0000] The "AIRFLOW_UID" variable is not set. Defaulting to a blank string. 
WARN[0000] The "AIRFLOW_UID" variable is not set. Defaulting to a blank string. 
WARN[0000] /Users/seilylook/Development/Book/Data_Engineering_with_Python/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 10/10
 âœ” Container postgres           Healthy                                                                                                          4.3s 
 âœ” Container elasticsearch      Healthy                                                                                                          3.6s 
 âœ” Container redis              Healthy                                                                                                          4.3s 
 âœ” Container kibana             Running                                                                                                          0.0s 
 âœ” Container airflow-init       Exited                                                                                                           7.8s 
 âœ” Container python-app         Started                                                                                                          3.8s 
 âœ” Container airflow-triggerer  Running                                                                                                          0.0s 
 âœ” Container airflow-webserver  Running                                                                                                          0.0s 
 âœ” Container airflow-scheduler  Running                                                                                                          0.0s 
 âœ” Container airflow-worker     Running                                                                                                          0.0s 


==============================================
Waiting for PostgreSQL to start...
==============================================
=====================================
Initializing PostgreSQL...
=====================================
chmod +x ./scripts/init_postgresql.sh
./scripts/init_postgresql.sh
dataengineering ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...
ERROR:  database "dataengineering" already exists
Successfully copied 2.05kB to postgres:/tmp/create_tables.sql
í…Œì´ë¸” ìƒì„± ì¤‘...
psql:/tmp/create_tables.sql:10: NOTICE:  relation "users" already exists, skipping
CREATE TABLE
        List of relations
 Schema | Name  | Type  |  Owner  
--------+-------+-------+---------
 public | users | table | airflow
(1 row)

                                    Table "public.users"
 Column |          Type          | Collation | Nullable |              Default              
--------+------------------------+-----------+----------+-----------------------------------
 id     | integer                |           | not null | nextval('users_id_seq'::regclass)
 name   | character varying(100) |           | not null | 
 street | character varying(200) |           |          | 
 city   | character varying(100) |           |          | 
 zip    | character varying(10)  |           |          | 
 lng    | numeric(10,6)          |           |          | 
 lat    | numeric(10,6)          |           |          | 
Indexes:
    "users_pkey" PRIMARY KEY, btree (id)

ê¶Œí•œ ë¶€ì—¬ ì¤‘...
GRANT
ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...
/var/run/postgresql:5432 - accepting connections
ë°ì´í„°ë² ì´ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.




==============================================
Waiting for Elasticsearch to start...
==============================================
=====================================
Initializing Elasticsearch...
=====================================
chmod +x ./scripts/init_elasticsearch.sh
./scripts/init_elasticsearch.sh
Elasticsearchê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘...
Elasticsearchê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.
users ì¸ë±ìŠ¤ ìƒì„± ì¤‘...
{"error":{"root_cause":[{"type":"resource_already_exists_exception","reason":"index [users/AW57UPmxTYyW3G-GdL6lHw] already exists","index_uuid":"AW57UPmxTYyW3G-GdL6lHw","index":"users"}],"type":"resource_already_exists_exception","reason":"index [users/AW57UPmxTYyW3G-GdL6lHw] already exists","index_uuid":"AW57UPmxTYyW3G-GdL6lHw","index":"users"},"status":400}users ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸:
health status index                                                              uuid                   pri rep docs.count docs.deleted store.size pri.store.size dataset.size
green  open   .internal.alerts-transform.health.alerts-default-000001            MSc-VAFyQHG9tGk2TxwbGg   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.logs.alerts-default-000001          Bief08evQ_SX_3UrwZOzwQ   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.uptime.alerts-default-000001        N-ptFAoNTYeF7OHGtOqZFw   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-ml.anomaly-detection.alerts-default-000001        JL9JINenTS-XMJClxKXrYA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.slo.alerts-default-000001           rFwL_h62QZmQx8kkXFZIyA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-default.alerts-default-000001                     6xFM1NqvTc--9BJ7MlhCpA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.apm.alerts-default-000001           D1xM5G2DTPeF25kkR6_KWQ   1   0          0            0       249b           249b         249b
green  open   users                                                              AW57UPmxTYyW3G-GdL6lHw   1   0       1000            0    229.1kb        229.1kb      229.1kb
green  open   .internal.alerts-observability.metrics.alerts-default-000001       IO4Li-8sS6-AH8aSsvEqLg   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-ml.anomaly-detection-health.alerts-default-000001 95sA140IQ2qCyErtmHKTEg   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.threshold.alerts-default-000001     TV3IWC9fQUuYWQO-56_5sA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-security.alerts-default-000001                    VE-qcLiURZy7h_i1hb96Lw   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-stack.alerts-default-000001                       _PEhTLb4TJ2bYAwiv6kz8w   1   0          0            0       249b           249b         249b
```

#### Make test

```bash
app-py3.12 {seilylook} ğŸ€ make test 
=======================
Running tests with pytest...
=======================
mkdir -p target
docker run --rm -v /Users/seilylook/Development/Book/Data_Engineering_with_Python/app/target:/app/target python-app:latest /bin/bash -c \
                'for test_file in $(find tests -name "*.py" ! -name "__init__.py"); do \
                        base_name=$(basename $test_file .py); \
                        pytest $test_file --junitxml=target/$base_name.xml; \
                done'
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 7 items

tests/test_progress_bar.py .......                                       [100%]

------------ generated xml file: /app/target/test_progress_bar.xml -------------
============================== 7 passed in 0.06s ===============================
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 9 items

tests/test_data_generator.py .........                                   [100%]

----------- generated xml file: /app/target/test_data_generator.xml ------------
============================== 9 passed in 0.03s ===============================
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 0 items

----------------- generated xml file: /app/target/conftest.xml -----------------
============================ no tests ran in 0.00s =============================
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 7 items

tests/test_postgres_connector.py .......                                 [100%]

--------- generated xml file: /app/target/test_postgres_connector.xml ----------
============================== 7 passed in 0.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 9 items

tests/test_database_config.py .........                                  [100%]

----------- generated xml file: /app/target/test_database_config.xml -----------
============================== 9 passed in 0.02s ===============================
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 14 items

tests/test_database_pytest.py ..............                             [100%]

----------- generated xml file: /app/target/test_database_pytest.xml -----------
============================== 14 passed in 0.19s ==============================
============================= test session starts ==============================
platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0
rootdir: /app
plugins: Faker-36.1.1, time-machine-2.16.0, anyio-4.8.0
collected 8 items

tests/test_elasticsearch_connector.py ........                           [100%]

------- generated xml file: /app/target/test_elasticsearch_connector.xml -------
============================== 8 passed in 0.18s ===============================
```

#### Make start

```bash
app-py3.12 {seilylook} ğŸ€ make start
=========================
Starting the application...
=========================
python -m src.main
2025-02-28 17:34:59,269 - root - INFO - ë°ì´í„° ìƒì„± ë° ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
2025-02-28 17:34:59,269 - root - INFO - ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: data/raw/test_data.csv
2025-02-28 17:34:59,285 - src.utils.connection - INFO - PostgreSQL ì—°ê²° ì„±ê³µ!
2025-02-28 17:34:59,286 - src.utils.connection - INFO - Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ
2025-02-28 17:34:59,290 - elastic_transport.transport - INFO - GET http://localhost:9200/ [status:200 duration:0.004s]
2025-02-28 17:34:59,290 - src.utils.connection - INFO - Elasticsearch ì—°ê²° ì„±ê³µ! ë²„ì „: 8.17.2
2025-02-28 17:34:59,290 - root - INFO - Postgresql ìƒíƒœ: ì—°ê²°ë¨
2025-02-28 17:34:59,290 - root - INFO - Elasticsearch ìƒíƒœ: ì—°ê²°ë¨
2025-02-28 17:34:59,295 - root - INFO - PostgreSQL: 1000ê°œ ë ˆì½”ë“œë¥¼ data/raw/test_data.csvì—ì„œ ì½ì—ˆìŠµë‹ˆë‹¤
2025-02-28 17:34:59,330 - src.database.repository - INFO - Bulk inserted 1000 users
2025-02-28 17:34:59,331 - root - INFO - PostgreSQLì— 1000ê°œ ë ˆì½”ë“œ ì €ì¥ ì™„ë£Œ
2025-02-28 17:34:59,331 - root - INFO - PostgreSQLì— 1000ê°œ ë ˆì½”ë“œ ì €ì¥ë¨
2025-02-28 17:34:59,333 - root - INFO - Elasticsearch: 1000ê°œ ë ˆì½”ë“œë¥¼ data/raw/test_data.csvì—ì„œ ì½ì—ˆìŠµë‹ˆë‹¤
2025-02-28 17:34:59,335 - src.utils.connection - INFO - Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ
2025-02-28 17:34:59,408 - elastic_transport.transport - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.068s]
2025-02-28 17:34:59,410 - src.database.repository - INFO - Elasticsearchì— 1000ê°œ ë¬¸ì„œ ë²Œí¬ ì €ì¥ ì™„ë£Œ
2025-02-28 17:34:59,410 - root - INFO - Elasticsearchì— 1000ê°œ ë ˆì½”ë“œ ì €ì¥ ì™„ë£Œ
2025-02-28 17:34:59,410 - root - INFO - Elasticsearchì— 1000ê°œ ë ˆì½”ë“œ ì €ì¥ë¨
2025-02-28 17:34:59,414 - root - INFO - PostgreSQLì—ì„œ 5ê°œ ë ˆì½”ë“œ ì¡°íšŒ ì™„ë£Œ
2025-02-28 17:34:59,414 - root - INFO - PostgreSQL ë°ì´í„° í™•ì¸ (ìƒ˜í”Œ 5ê°œ):
2025-02-28 17:34:59,414 - root - INFO -   ë ˆì½”ë“œ 1: {'id': 1, 'name': 'Whitney Olson', 'street': '1791 Pittman Overpass', 'city': 'Lake Jason', 'zip': '48870', 'lng': Decimal('114.735089'), 'lat': Decimal('45.235433')}
2025-02-28 17:34:59,414 - root - INFO -   ë ˆì½”ë“œ 2: {'id': 2, 'name': 'David Smith', 'street': '0474 Julian Station', 'city': 'West Sophia', 'zip': '72976', 'lng': Decimal('94.204753'), 'lat': Decimal('-88.761862')}
2025-02-28 17:34:59,414 - root - INFO -   ë ˆì½”ë“œ 3: {'id': 3, 'name': 'Mr. Jason Hughes MD', 'street': '7351 Robinson Underpass', 'city': 'Stephaniebury', 'zip': '8702', 'lng': Decimal('-87.282108'), 'lat': Decimal('12.763472')}
2025-02-28 17:34:59,414 - root - INFO -   ë ˆì½”ë“œ 4: {'id': 4, 'name': 'John Johnson', 'street': '8304 Cooper Mews', 'city': 'Candicefort', 'zip': '87821', 'lng': Decimal('-169.562279'), 'lat': Decimal('-53.845951')}
2025-02-28 17:34:59,414 - root - INFO -   ë ˆì½”ë“œ 5: {'id': 5, 'name': 'Gregory Harrison', 'street': '0866 Lee Expressway Suite 888', 'city': 'Dianaport', 'zip': '14219', 'lng': Decimal('-30.874919'), 'lat': Decimal('84.261251')}
2025-02-28 17:34:59,414 - src.utils.connection - INFO - Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ
2025-02-28 17:34:59,419 - elastic_transport.transport - INFO - POST http://localhost:9200/users/_search [status:200 duration:0.005s]
2025-02-28 17:34:59,420 - root - INFO - Elasticsearchì—ì„œ 5ê°œ ë ˆì½”ë“œ ì¡°íšŒ ì™„ë£Œ
2025-02-28 17:34:59,420 - root - INFO - Elasticsearch ë°ì´í„° í™•ì¸ (ìƒ˜í”Œ 5ê°œ):
2025-02-28 17:34:59,420 - root - INFO -   ë ˆì½”ë“œ 1: {'name': 'Whitney Olson', 'age': 26, 'street': '1791 Pittman Overpass', 'city': 'Lake Jason', 'state': 'Idaho', 'zip': 48870, 'lng': 114.735089, 'lat': 45.2354325}
2025-02-28 17:34:59,420 - root - INFO -   ë ˆì½”ë“œ 2: {'name': 'David Smith', 'age': 28, 'street': '0474 Julian Station', 'city': 'West Sophia', 'state': 'Arizona', 'zip': 72976, 'lng': 94.204753, 'lat': -88.761862}
2025-02-28 17:34:59,420 - root - INFO -   ë ˆì½”ë“œ 3: {'name': 'Mr. Jason Hughes MD', 'age': 70, 'street': '7351 Robinson Underpass', 'city': 'Stephaniebury', 'state': 'Mississippi', 'zip': 8702, 'lng': -87.282108, 'lat': 12.763472}
2025-02-28 17:34:59,420 - root - INFO -   ë ˆì½”ë“œ 4: {'name': 'John Johnson', 'age': 41, 'street': '8304 Cooper Mews', 'city': 'Candicefort', 'state': 'Rhode Island', 'zip': 87821, 'lng': -169.562279, 'lat': -53.845951}
2025-02-28 17:34:59,420 - root - INFO -   ë ˆì½”ë“œ 5: {'name': 'Gregory Harrison', 'age': 24, 'street': '0866 Lee Expressway Suite 888', 'city': 'Dianaport', 'state': 'New Jersey', 'zip': 14219, 'lng': -30.874919, 'lat': 84.261251}
2025-02-28 17:34:59,420 - root - INFO - ë°ì´í„° ìƒì„± ë° ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ
```

## Chapter 3. Reading and Writing Files

### Handling files using NiFi processors

ì•ì„œ `make start`ë¥¼ í†µí•´ Fakerë¥¼ í™œìš©í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤. ì´ ë°ì´í„°ëŠ” ë‹¤ìŒ ë””ë ‰í† ë¦¬ì— ì¡´ì¬í•œë‹¤.

- local: /app/data/raw/test_data.csv

- NiFi container: /opt/nifi/nifi-current/data/raw/test_data.csv

```bash
# Nifi container ì‹¤í–‰
app-py3.12 {seilylook} ğŸš€ docker exec -i -t nifi /bin/bash

# Containerì— ì›ë³¸ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
nifi@e92527995ead:/opt/nifi/nifi-current$ ls data/raw/
test_data.csv

# 40ì„¸ ì´ìƒ ì‚¬ëŒë“¤ Query í•˜ê³  Name ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
nifi@e92527995ead:/opt/nifi/nifi-current/data/processed$ ls -al
total 84
drwxr-xr-x 12 nifi nifi  384 Mar  7 07:17  .
drwxr-xr-x  4 root root 4096 Mar  7 07:05  ..
-rw-r--r--  1 nifi nifi 5637 Mar  7 07:17 'Amber Taylor'
-rw-r--r--  1 nifi nifi 5284 Mar  7 07:17 'Charles Arnold'
-rw-r--r--  1 nifi nifi 5789 Mar  7 07:17 'Corey Hardin'
-rw-r--r--  1 nifi nifi 6580 Mar  7 07:17 'Ebony Miller'
-rw-r--r--  1 nifi nifi 6030 Mar  7 07:17 'Grant Garrison'
-rw-r--r--  1 nifi nifi 5108 Mar  7 07:17 'Kristina Parker'
-rw-r--r--  1 nifi nifi 5444 Mar  7 07:17 'Nicholas Baker MD'
-rw-r--r--  1 nifi nifi 5277 Mar  7 07:17 'Phillip Love'
-rw-r--r--  1 nifi nifi 6180 Mar  7 07:17 'Whitney Barnes'
-rw-r--r--  1 nifi nifi 5438 Mar  7 07:17 'Zachary Cohen'
```

#### Processors ì„¤ì •

ì±… 63pë¥¼ ì°¸ì¡°í•´ì„œ Processorsì™€ ê° Processorsë“¤ì˜ Propertiesë¥¼ ì„¤ì •í–ˆë‹¤. ì›ë³¸ ë°ì´í„°ëŠ” Rowê°€ 1000ê°œì´ë‹¤. SplitRecord Processorì˜ `Records Per Split`ë¥¼ 100ìœ¼ë¡œ ì„¤ì •í–ˆê¸° ë•Œë¬¸ì— /opt/nifi/nifi-current/data/processed ì— ìˆëŠ” ê²°ê³¼ íŒŒì¼ë“¤ì„ ë³´ë©´ 10ê°œì„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

#### ë¬¸ì œ ë° í•´ê²°

1. Nifi container ìƒì„± 

ì´ˆê¸°ì— apache/nifi:latest ë²„ì „ìœ¼ë¡œ image pullì„ ìˆ˜í–‰í•˜ë‹ˆ version 2ë¶€í„° ìë™ìœ¼ë¡œ `https:`ë¡œ ì—°ê²°ë˜ë„ë¡ ì •ì˜ë˜ì–´ ìˆì—ˆë‹¤. ê·¸ë˜ì„œ ì´ì „ê¹Œì§€ Nifië¥¼ ë¹¼ê³  ë‚˜ë¨¸ì§€(postgresql, elasticsearch, airflow)ë§Œì„ docker containerë¡œ ë§Œë“¤ì—ˆìœ¼ë‚˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì‹¶ì–´ ì—¬ëŸ¬ê°€ì§€ ì‹œë„ë¥¼ í•˜ë‹¤ê°€ ë§ˆì§€ë§‰ ìˆ˜ë‹¨ìœ¼ë¡œ versionì„ 1.28.0(ë²„ì „ì„ ë‚®ì¶œ ë–„ ì£¼ì˜í•  ì ì€ OS/ARCH == linux/arm64 ë¥¼ ì§€ì›í•˜ëŠ” docker image ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤. ë‚´ MACì€ arm64ì´ê¸° ë•Œë¬¸ì´ë‹¤.) ìœ¼ë¡œ ë‚®ì¶”ë‹ˆ ì •ìƒì ìœ¼ë¡œ `http:` portë¥¼ ì‚¬ìš©í•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆì—ˆë‹¤.

2. Nifi Processor

Nifi Processorë¥¼ ìƒì„±í•˜ê³  Propertiesë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì€ ì±…ê³¼ ë™ì¼í•´ í° ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ë‹¤. ê·¸ëŸ°ë° ê³„ì†í•´ì„œ SplitRecord ë¶€ë¶„ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ë°, ì›ì¸ì€ `RELATIONS` ì„¤ì •. ì¦‰, ê°ê°ì˜ ProcessorëŠ” ì›í•˜ëŠ” ì—°ê²° ex, success, splits, over . 40, matched ë¿ë§Œ ì•„ë‹ˆë¼ failure, unmatched ë“± ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì—ì„œ ëŒ€í•´ì„œ **terminate** | **retry** ë¥¼ ì„¤ì •í•´ì£¼ì–´ì•¼ í•œë‹¤. ì‰½ê²Œ ìƒê°í•˜ë©´ ìƒë‹¨ì˜ `!(Warning)`ì´ í•˜ë‚˜ë„ ì—†ì–´ì•¼ í•œë‹¤.

## Chapter 4. Working with Databases

#### Inserting data into Databases

`make build`ë¥¼ ì‹¤í–‰í•˜ë©´ Makefileì—ì„œ ì‘ì„±í•œ ëŒ€ë¡œ `init_elasticsearch.sh`, `init_postgresql.sh`ë¥¼ ì‹¤í–‰ ì‹œí‚¨ë‹¤. 

- init_elasticsearch.sh

```shell
#!/bin/bash

# Elasticsearch ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
if ! docker ps | grep -q "elasticsearch"; then
    echo "Elasticsearch ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    exit 1
fi

# Elasticsearchê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
echo "Elasticsearchê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘..."
until $(curl --silent --output /dev/null --fail --max-time 5 http://localhost:9200); do
    printf '.'
    sleep 5
done
echo "Elasticsearchê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤."

# users ì¸ë±ìŠ¤ ì„¤ì •
echo "users ì¸ë±ìŠ¤ ìƒì„± ì¤‘..."
curl -X PUT "localhost:9200/users" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "street": { "type": "text" },
      "city": { "type": "text" },
      "zip": { "type": "keyword" },
      "lng": { "type": "double" },
      "lat": { "type": "double" }
    }
  }
}
'

# ê²°ê³¼ í™•ì¸
if [ $? -eq 0 ]; then
    echo "users ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
else
    echo "users ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    exit 1
fi

# ì¸ë±ìŠ¤ í™•ì¸
echo "ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸:"
curl -X GET "localhost:9200/_cat/indices?v"
```

- init_postgresql.sh

```shell
#!/bin/bash

# PostgreSQL ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
if ! docker ps | grep -q "postgres"; then
    echo "PostgreSQL ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    exit 1
fi

# ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
echo "dataengineering ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘..."
docker exec -i postgres bash -c 'PGPASSWORD=airflow psql -U airflow -d airflow -c "CREATE DATABASE dataengineering;"'

# ì´ì œ dataengineering ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸” ìƒì„±
cat << 'EOF' > /tmp/create_tables.sql
-- users í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    street VARCHAR(200),
    city VARCHAR(100),
    zip VARCHAR(10),
    lng DECIMAL(10, 6),
    lat DECIMAL(10, 6)
);

-- í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
\dt
-- í…Œì´ë¸” êµ¬ì¡° í™•ì¸
\d users
EOF

# PostgreSQL ì»¨í…Œì´ë„ˆì— SQL íŒŒì¼ ë³µì‚¬
docker cp /tmp/create_tables.sql postgres:/tmp/create_tables.sql

# PostgreSQL ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ SQL ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (dataengineering ë°ì´í„°ë² ì´ìŠ¤ì— ì§ì ‘ ì—°ê²°)
echo "í…Œì´ë¸” ìƒì„± ì¤‘..."
docker exec -i postgres bash -c 'PGPASSWORD=airflow psql -U airflow -d dataengineering -f /tmp/create_tables.sql'

# ê¶Œí•œ ë¶€ì—¬
echo "ê¶Œí•œ ë¶€ì—¬ ì¤‘..."
docker exec -i postgres bash -c 'PGPASSWORD=airflow psql -U airflow -d dataengineering -c "GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow; GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO airflow;"'

# ì‹¤í–‰ ê²°ê³¼ í™•ì¸
if [ $? -eq 0 ]; then
    echo "ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    rm /tmp/create_tables.sql
    docker exec postgres rm /tmp/create_tables.sql
else
    echo "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    rm /tmp/create_tables.sql
    docker exec postgres rm /tmp/create_tables.sql
    exit 1
fi

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
echo "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
docker exec postgres pg_isready -U airflow -d dataengineering

if [ $? -eq 0 ]; then
    echo "ë°ì´í„°ë² ì´ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤."
else
    echo "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    exit 1
fi
```

ì´ë¥¼ í†µí•´ ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ë‘ê°œì˜ ë°ì´í„°ë¥¼ **PostgresSQL**, **ElasticSearch**ì— ê°ê° ì €ì¥í•œë‹¤. ì•„ë˜ì˜ Logì„ í†µí•´ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```bash
==============================================
Waiting for PostgreSQL to start...
==============================================
=====================================
Initializing PostgreSQL...
=====================================
chmod +x ./scripts/init_postgresql.sh
./scripts/init_postgresql.sh
dataengineering ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...
ERROR:  database "dataengineering" already exists
Successfully copied 2.05kB to postgres:/tmp/create_tables.sql
í…Œì´ë¸” ìƒì„± ì¤‘...
psql:/tmp/create_tables.sql:10: NOTICE:  relation "users" already exists, skipping
CREATE TABLE
        List of relations
 Schema | Name  | Type  |  Owner  
--------+-------+-------+---------
 public | users | table | airflow
(1 row)

                                    Table "public.users"
 Column |          Type          | Collation | Nullable |              Default              
--------+------------------------+-----------+----------+-----------------------------------
 id     | integer                |           | not null | nextval('users_id_seq'::regclass)
 name   | character varying(100) |           | not null | 
 street | character varying(200) |           |          | 
 city   | character varying(100) |           |          | 
 zip    | character varying(10)  |           |          | 
 lng    | numeric(10,6)          |           |          | 
 lat    | numeric(10,6)          |           |          | 
Indexes:
    "users_pkey" PRIMARY KEY, btree (id)

ê¶Œí•œ ë¶€ì—¬ ì¤‘...
GRANT
ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...
/var/run/postgresql:5432 - accepting connections
ë°ì´í„°ë² ì´ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.




==============================================
Waiting for Elasticsearch to start...
==============================================
=====================================
Initializing Elasticsearch...
=====================================
chmod +x ./scripts/init_elasticsearch.sh
./scripts/init_elasticsearch.sh
Elasticsearchê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘...
Elasticsearchê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.
users ì¸ë±ìŠ¤ ìƒì„± ì¤‘...
{"error":{"root_cause":[{"type":"resource_already_exists_exception","reason":"index [users/AW57UPmxTYyW3G-GdL6lHw] already exists","index_uuid":"AW57UPmxTYyW3G-GdL6lHw","index":"users"}],"type":"resource_already_exists_exception","reason":"index [users/AW57UPmxTYyW3G-GdL6lHw] already exists","index_uuid":"AW57UPmxTYyW3G-GdL6lHw","index":"users"},"status":400}users ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸:
health status index                                                              uuid                   pri rep docs.count docs.deleted store.size pri.store.size dataset.size
green  open   .internal.alerts-transform.health.alerts-default-000001            MSc-VAFyQHG9tGk2TxwbGg   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.logs.alerts-default-000001          Bief08evQ_SX_3UrwZOzwQ   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.uptime.alerts-default-000001        N-ptFAoNTYeF7OHGtOqZFw   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-ml.anomaly-detection.alerts-default-000001        JL9JINenTS-XMJClxKXrYA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.slo.alerts-default-000001           rFwL_h62QZmQx8kkXFZIyA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-default.alerts-default-000001                     6xFM1NqvTc--9BJ7MlhCpA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.apm.alerts-default-000001           D1xM5G2DTPeF25kkR6_KWQ   1   0          0            0       249b           249b         249b
green  open   users                                                              AW57UPmxTYyW3G-GdL6lHw   1   0       1000            0    229.1kb        229.1kb      229.1kb
green  open   .internal.alerts-observability.metrics.alerts-default-000001       IO4Li-8sS6-AH8aSsvEqLg   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-ml.anomaly-detection-health.alerts-default-000001 95sA140IQ2qCyErtmHKTEg   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-observability.threshold.alerts-default-000001     TV3IWC9fQUuYWQO-56_5sA   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-security.alerts-default-000001                    VE-qcLiURZy7h_i1hb96Lw   1   0          0            0       249b           249b         249b
green  open   .internal.alerts-stack.alerts-default-000001                       _PEhTLb4TJ2bYAwiv6kz8w   1   0          0            0       249b           249b         249b
```

#### Create index pattern in ElasticSearch

PostgreSQL UIë¥¼ ìœ„í•œ pgAdminì€ ë”°ë¡œ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— `psql`ì„ í†µí•´ í™•ì¸í•˜ëŠ” ê²ƒìœ¼ë¡œ í•˜ê³  ElasticSearchì— ë°ì´í„°ê°€ ì ì ˆíˆ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì•ì„œ ì„¤ì¹˜í•œ `Kibana`ë¥¼ ì´ìš©í•œë‹¤. init_elasticsearch.shì—ì„œ `users` indexë¥¼ ìƒì„±í•˜ê³  ì´ˆê¸° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì €ì¥í•´ì£¼ì—ˆë‹¤.

http://localhost:5601ì— ì ‘ê·¼í•˜ê³ , ë‹¤ìŒì˜ ê³¼ì •ì„ í†µí•´ì„œ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ElasticSearchì— ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì¤€ë‹¤.

1. ì™¼ìª½ Toolbarì—ì„œ **Analytics** -> **Discover** í´ë¦­

<img src="./images/Figure 4.1.png" />

2. **Create index pattern** í´ë¦­

<img src="./images/Figure 4.2.png" />

3. ì˜¤ë¥¸ìª½ í™”ë©´ì„ ë³´ë©´ ì•ì„œ init_elasticsearch.shë¥¼ í†µí•´ ìƒì„±í•œ users indexê°€ ë³´ì¸ë‹¤. ê·¸ë ‡ê¸°ì— ì™¼ìª½ì— **Name**ì— **uses**ë¥¼ ë„£ì–´ì¤€ë‹¤.

<img src="./images/Figure 4.3.png" />

4. indexê°€ ì—°ê²°ë˜ì—ˆê¸°ì— **field type**ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<img src="./images/Figure 4.4.png" />

5. ì™¼ìª½ Toolbardptj **Discover** í´ë¦­. ì €ì¥ëœ ê°’ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<img src="./images/Figure 4.5.png" />

### Building data pipelines in Apache Airflow

1. DAG ì‘ì„±

```python
import os
import datetime as dt
from datetime import timedelta
import logging
import pandas as pd
import psycopg2
from psycopg2.extras import DictCursor

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
from elasticsearch import Elasticsearch

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹œë„ - ì„í¬íŠ¸ ê²½ë¡œ ë¬¸ì œ í•´ê²°
import sys
import importlib.util
import inspect

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ
current_file = inspect.getfile(inspect.currentframe())
# í˜„ì¬ ë””ë ‰í† ë¦¬ (airflow/dags)
current_dir = os.path.dirname(os.path.abspath(current_file))
# Airflow í™ˆ ë””ë ‰í† ë¦¬ (/opt/airflow)
airflow_home = os.path.dirname(os.path.dirname(current_dir))
# app ë””ë ‰í† ë¦¬ ê²½ë¡œ
app_dir = os.path.join(airflow_home, "app")

# Python ê²½ë¡œì— app ë””ë ‰í† ë¦¬ ì¶”ê°€
if app_dir not in sys.path:
    sys.path.insert(0, app_dir)
    logger.info(f"Python ê²½ë¡œì— ì¶”ê°€ë¨: {app_dir}")

# ê²½ë¡œ ë””ë²„ê¹…
logger.info(f"Python ê²½ë¡œ: {sys.path}")
logger.info(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
logger.info(f"í˜„ì¬ íŒŒì¼: {current_file}")
logger.info(f"app ë””ë ‰í† ë¦¬ ê²½ë¡œ: {app_dir}")

# ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹œë„
USE_REPOSITORY_PATTERN = False
try:
    from src.database.repository import RepositoryFactory
    from src.config.database import PostgresConfig, ElasticsearchConfig
    from src.utils.connection import PostgresConnector, ElasticsearchConnector

    # ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ëŠ”ì§€ í™•ì¸
    if all(
        [
            RepositoryFactory,
            PostgresConfig,
            ElasticsearchConfig,
            PostgresConnector,
            ElasticsearchConnector,
        ]
    ):
        USE_REPOSITORY_PATTERN = True
        logger.info("Repository ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
except ImportError as e:
    logger.error(f"Repository ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    logger.info("ì§ì ‘ êµ¬í˜„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")

# DAG ì„¤ì •
default_args = {
    "owner": "Se Hyeon Kim",
    "start_date": dt.datetime(2025, 3, 1),
    "retries": 3,
    "retry_delay": dt.timedelta(minutes=5),
    "email_on_failure": True,
    "email_on_retry": False,
}

# PostgreSQL ì—°ê²° ì •ë³´
PG_HOST = "postgres"  # Docker Composeì˜ ì„œë¹„ìŠ¤ ì´ë¦„
PG_PORT = 5432
PG_DATABASE = "airflow"
PG_USER = "airflow"
PG_PASSWORD = "airflow"

# Elasticsearch ì—°ê²° ì •ë³´
ES_HOST = "elasticsearch"  # Docker Composeì˜ ì„œë¹„ìŠ¤ ì´ë¦„
ES_PORT = 9200


def extract_from_postgresql(**context):
    """PostgreSQLì—ì„œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ CSV íŒŒì¼ë¡œ ì €ì¥"""
    global USE_REPOSITORY_PATTERN  # ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸

    try:
        if USE_REPOSITORY_PATTERN:
            # Repository íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì ‘ê·¼
            try:
                logger.info("Repository íŒ¨í„´ìœ¼ë¡œ PostgreSQL ì ‘ê·¼ ì‹œë„")
                postgres_config = PostgresConfig()
                postgres_connector = PostgresConnector(config=postgres_config)
                repository = RepositoryFactory.create(
                    "postgresql", connector=postgres_connector
                )

                # ì—°ê²° í™•ì¸
                if not repository.check_connection():
                    raise Exception("PostgreSQL ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                # ë°ì´í„° ì¡°íšŒ
                users = repository.get_all(limit=1000)

                if not users:
                    logger.warning("PostgreSQLì—ì„œ ì¡°íšŒëœ ì‚¬ìš©ì ì—†ìŒ")
                    return False

                # DataFrameìœ¼ë¡œ ë³€í™˜
                df = pd.DataFrame(users)
                logger.info(
                    f"Repository íŒ¨í„´ìœ¼ë¡œ {len(df)}ëª…ì˜ ì‚¬ìš©ì ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ"
                )
            except Exception as repo_error:
                logger.error(f"Repository íŒ¨í„´ ì‚¬ìš© ì‹¤íŒ¨: {repo_error}")
                logger.info("ì§ì ‘ DB ì—°ê²°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
                # Repository íŒ¨í„´ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ì—°ê²°ë¡œ ëŒ€ì²´
                USE_REPOSITORY_PATTERN = False
                raise repo_error

        if not USE_REPOSITORY_PATTERN:
            # ì§ì ‘ psycopg2ë¡œ PostgreSQLì— ì—°ê²°
            conn_string = f"host={PG_HOST} port={PG_PORT} dbname={PG_DATABASE} user={PG_USER} password={PG_PASSWORD}"
            logger.info(f"PostgreSQL ì§ì ‘ ì—°ê²°: {conn_string}")

            conn = psycopg2.connect(conn_string)

            with conn.cursor(cursor_factory=DictCursor) as cur:
                # users í…Œì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ë¥¸ í…Œì´ë¸” ì‚¬ìš© (ì˜ˆ: dag_run)
                cur.execute("SELECT * FROM dag_run LIMIT 100")
                rows = cur.fetchall()

                if not rows:
                    logger.warning("PostgreSQLì—ì„œ ì¡°íšŒëœ ë°ì´í„° ì—†ìŒ")
                    return False

                # ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                data = [dict(row) for row in rows]
                df = pd.DataFrame(data)

            conn.close()

        # CSV ì €ì¥
        output_path = "/tmp/postgresql_users.csv"
        df.to_csv(output_path, index=False)
        context["ti"].xcom_push(key="csv_path", value=output_path)

        logger.info(f"PostgreSQLì—ì„œ {len(df)}ê°œì˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"PostgreSQL ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ DB ì—°ê²°ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ë°±ì—…)
        try:
            logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤")
            test_data = [
                {
                    "id": 1,
                    "name": "User 1",
                    "street": "Street 1",
                    "city": "City 1",
                    "zip": "11111",
                    "lng": "1.1",
                    "lat": "1.1",
                },
                {
                    "id": 2,
                    "name": "User 2",
                    "street": "Street 2",
                    "city": "City 2",
                    "zip": "22222",
                    "lng": "2.2",
                    "lat": "2.2",
                },
                {
                    "id": 3,
                    "name": "User 3",
                    "street": "Street 3",
                    "city": "City 3",
                    "zip": "33333",
                    "lng": "3.3",
                    "lat": "3.3",
                },
            ]
            df = pd.DataFrame(test_data)
            output_path = "/tmp/postgresql_users.csv"
            df.to_csv(output_path, index=False)
            context["ti"].xcom_push(key="csv_path", value=output_path)
            logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ")
            return True
        except Exception as test_error:
            logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {test_error}")
            raise AirflowException(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")


def load_to_elasticsearch(**context):
    """CSV íŒŒì¼ ë°ì´í„°ë¥¼ Elasticsearchì— ì ì¬"""
    global USE_REPOSITORY_PATTERN  # ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸

    try:
        # CSV íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        ti = context["ti"]
        csv_path = ti.xcom_pull(task_ids="extract_postgresql_data", key="csv_path")

        if not csv_path:
            raise AirflowException(
                "ì´ì „ íƒœìŠ¤í¬ì—ì„œ CSV íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(csv_path)

        if df.empty:
            logger.warning("ì ì¬í•  ì‚¬ìš©ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False

        if USE_REPOSITORY_PATTERN:
            # Repository íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ Elasticsearchì— ì ì¬
            try:
                logger.info("Repository íŒ¨í„´ìœ¼ë¡œ Elasticsearch ì ‘ê·¼ ì‹œë„")
                es_config = ElasticsearchConfig()
                es_connector = ElasticsearchConnector(config=es_config)
                repository = RepositoryFactory.create(
                    "elasticsearch",
                    connector=es_connector,
                    index="users_from_postgresql",
                )

                # ì—°ê²° í™•ì¸
                if not repository.check_connection():
                    raise Exception("Elasticsearch ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                # ë°ì´í„° ë³€í™˜ ë° ì ì¬
                records = df.to_dict("records")
                inserted_count = repository.bulk_save(records)

                logger.info(f"Elasticsearchì— {inserted_count}ê°œ ë¬¸ì„œ ì ì¬ ì™„ë£Œ")
                return True
            except Exception as repo_error:
                logger.error(f"Repository íŒ¨í„´ ì‚¬ìš© ì‹¤íŒ¨: {repo_error}")
                logger.info("ì§ì ‘ Elasticsearch ì—°ê²°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
                # ì‹¤íŒ¨ ì‹œ ì§ì ‘ ì—°ê²°ë¡œ ëŒ€ì²´
                USE_REPOSITORY_PATTERN = False
                raise repo_error

        if not USE_REPOSITORY_PATTERN:
            # Elasticsearch ì§ì ‘ ì—°ê²°
            es_url = f"http://{ES_HOST}:{ES_PORT}"
            logger.info(f"Elasticsearch ì§ì ‘ ì—°ê²°: {es_url}")

            es = Elasticsearch([es_url])

            # ì—°ê²° í™•ì¸
            if not es.ping():
                logger.error("Elasticsearch ì—°ê²° ì‹¤íŒ¨")
                logger.info(
                    "ì‘ì—…ì„ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤ (ì‹¤ì œ ë°ì´í„°ëŠ” ì ì¬ë˜ì§€ ì•ŠìŒ)"
                )
                return True  # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬

            # ë°ì´í„° ë³€í™˜ ë° ì ì¬
            index_name = "users_from_postgresql"
            bulk_data = []
            for _, row in df.iterrows():
                bulk_data.append({"index": {"_index": index_name}})
                bulk_data.append(row.to_dict())

            if bulk_data:
                es.bulk(operations=bulk_data, refresh=True)

            logger.info(f"Elasticsearchì— {len(df)}ê°œ ë¬¸ì„œ ì ì¬ ì™„ë£Œ")
            return True

    except Exception as e:
        logger.error(f"Elasticsearch ë°ì´í„° ì ì¬ ì‹¤íŒ¨: {e}")
        # ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ì´ ì˜¤ë¥˜ë¥¼ ë¬´ì‹œí•˜ê³  ì§„í–‰
        logger.info("Elasticsearch ì ì¬ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ì‘ì—…ì„ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤")
        return True


# DAG ì •ì˜
with DAG(
    dag_id="user_data_transfer",
    default_args=default_args,
    description="PostgreSQL ì‚¬ìš©ì ë°ì´í„°ë¥¼ Elasticsearchë¡œ ì „ì†¡",
    schedule_interval=timedelta(hours=1),
    catchup=False,
    tags=["postgresql", "elasticsearch", "user_data"],
) as dag:

    extract_task = PythonOperator(
        task_id="extract_postgresql_data",
        python_callable=extract_from_postgresql,
    )

    load_task = PythonOperator(
        task_id="load_elasticsearch_data",
        python_callable=load_to_elasticsearch,
    )

    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    extract_task >> load_task
```

2. ì‹¤í–‰ ê²°ê³¼

<img src="./images/Figure 4.6.png" />

<img src="./images/Figure 4.7.png" />

<img src="./images/Figure 4.8.png" />

- extract_postgresql_data

```bash
ac7a746c901c
 â–¶ Log message source details
[2025-03-08, 04:04:08 UTC] {local_task_job_runner.py:123} â–¶ Pre task execution logs
[2025-03-08, 04:04:08 UTC] {main.py:128} INFO - PostgreSQL ì§ì ‘ ì—°ê²°: host=postgres port=5432 dbname=*** user=*** password=***
[2025-03-08, 04:04:08 UTC] {main.py:152} INFO - PostgreSQLì—ì„œ 1ê°œì˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ
[2025-03-08, 04:04:08 UTC] {python.py:240} INFO - Done. Returned value was: True
[2025-03-08, 04:04:08 UTC] {taskinstance.py:341} â–¶ Post task execution logs
```

- load_elasticsearch_data

```bash
ac7a746c901c
 â–¶ Log message source details
[2025-03-08, 04:04:09 UTC] {local_task_job_runner.py:123} â–¶ Pre task execution logs
[2025-03-08, 04:04:09 UTC] {main.py:253} INFO - Elasticsearch ì§ì ‘ ì—°ê²°: http://elasticsearch:9200
[2025-03-08, 04:04:09 UTC] {_transport.py:349} INFO - HEAD http://elasticsearch:9200/ [status:200 duration:0.003s]
[2025-03-08, 04:04:09 UTC] {_transport.py:349} INFO - PUT http://elasticsearch:9200/_bulk?refresh=true [status:200 duration:0.096s]
[2025-03-08, 04:04:09 UTC] {main.py:275} INFO - Elasticsearchì— 1ê°œ ë¬¸ì„œ ì ì¬ ì™„ë£Œ
[2025-03-08, 04:04:09 UTC] {python.py:240} INFO - Done. Returned value was: True
[2025-03-08, 04:04:09 UTC] {taskinstance.py:341} â–¶ Post task execution logs
```

### Handling databases with NiFi processorss

#### Extracting data from PostgreSQL

<img src="./images/Figure 4.9.png" />

```bash
app-py3.12 {seilylook} â˜•ï¸ curl -X GET "localhost:9200/_cat/indices?v"                                          
health status index                            uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   .geoip_databases                 7wlLmCbzT-68QQtczL1XlQ   1   0         39            0     36.2mb         36.2mb
green  open   .kibana_task_manager_7.17.28_001 32xC4NKOQCWKlK_Lta3KuA   1   0         17         1206    267.3kb        267.3kb
yellow open   fromnifi                         7wEUB-_YTI6ZpVo9nVu0Fg   1   1     450400            0    189.9mb        189.9mb
green  open   .kibana_7.17.28_001              lUIvEKPPQwiGzG94W_LaVA   1   0         15            0      2.3mb          2.3mb
green  open   .apm-custom-link                 guOxO9XJSdmOvvEuTyOdnA   1   0          0            0       227b           227b
green  open   .apm-agent-configuration         77OEbeUsSsW8bGctTzmD1A   1   0          0            0       227b           227b
green  open   users                            iI4uC4UiRPyFBgts1aAB7w   1   0       1000            0    231.9kb        231.9kb
```

#### ë¬¸ì œ í•´ê²°

Docker í™˜ê²½ì—ì„œ Postgres, Elasticsearch, Nifië¥¼ ì‹¤í–‰í•˜ëŠ” ìƒíƒœì´ê¸° ë•Œë¬¸ì— ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ë¥´ê²Œ ë”°ë¡œ ì„¸íŒ…ì´ í•„ìš”í•˜ë‹¤. 

PostgreSQL, Elasticsearch connection í•  ë•Œ, postgresëŠ” jarë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤. PostgreSQL ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë²„ì „ì˜ jarì„ ë‹¤ìš´ë°›ì•„ localì˜ nifi/driversì— ì €ì¥í•´ì¤¬ë‹¤. ì´ë¥¼ docker-composeì—ì„œ nifi containerë¥¼ ë§Œë“¤ ë•Œ `mount` ì‹œì¼œì„œ nifi containerì—ì„œ PostgreSQL jarë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤.

```yaml
services:
  nifi:
    image: apache/nifi:1.28.0
    container_name: nifi
    restart: always
    ports:
      - "9300:9300"
    environment:
      - NIFI_WEB_HTTP_HOST=0.0.0.0
      - NIFI_WEB_HTTP_PORT=9300
      - NIFI_WEB_PROXY_HOST=localhost:9300
      - SINGLE_USER_CREDENTIALS_USERNAME=nifi
      - SINGLE_USER_CREDENTIALS_PASSWORD=nifipassword
    volumes:
      - nifi-system-data:/opt/nifi/nifi-current/system-data  # ë‚´ë¶€ ì‹œìŠ¤í…œ ë°ì´í„°
      - ./logs/nifi:/opt/nifi/nifi-current/logs
      - nifi-conf:/opt/nifi/nifi-current/conf
      - ./nifi/data/raw:/opt/nifi/nifi-current/data/raw      # ì›ë³¸ ë°ì´í„°
      - ./nifi/data/processed:/opt/nifi/nifi-current/data/processed  # ì²˜ë¦¬ëœ ë°ì´í„°
      - ./nifi/templates:/opt/nifi/nifi-current/templates
      - ./nifi/drivers:/opt/nifi/nifi-current/lib/custom-drivers
```

ì˜¬ë°”ë¥´ê²Œ mount ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì¤€ë‹¤.

```bash
app-py3.12 {seilylook} â˜•ï¸ docker exec -i -t nifi /bin/bash    

nifi@d8ffbc7cb03c:/opt/nifi/nifi-current$ cd lib/custom-drivers/
nifi@d8ffbc7cb03c:/opt/nifi/nifi-current/lib/custom-drivers$ ls
postgresql-42.7.5.jar
```

ì´ì–´ì„œ **DBCPConnectionPool** ì„œë¹„ìŠ¤ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•´ì¤€ë‹¤.

- Database Connection URL:

> jdbc:postgresql://postgres:5432/dataengineering

- Database Driver Class Name:

> org.postgresql.Driver

- Database Driver Locations:

> /opt/nifi/nifi-current/lib/custom-drivers/postgresql-42.7.1.jar

- Database User:

> airflow

- Password:

> airflow

ë§ˆì§€ë§‰ìœ¼ë¡œ **PutElasticsearchHtttp** processorì˜ propertiesì—ì„œ URLì™€ Portë¥¼ ë‹¤ìŒìœ¼ë¡œ ì„¤ì •í•´ì¤€ë‹¤.

> http://elasticsearch:9200

## Chapter 6. Building a 311 Data Pipeline

### Building the data pipeline

#### NiFi data extraction & Load ë¬¸ì œ í•´ê²° ê³¼ì •

##### ë¬¸ì œ ìƒí™©

NiFi ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤:

1. ExecuteScript (1ì°¨): ì™¸ë¶€ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ìŠ¤í¬ë¦½íŠ¸
2. SplitJSON: ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ ê°œë³„ ë ˆì½”ë“œë¡œ ë¶„í• 
3. ExecuteScript (2ì°¨): ê° ë ˆì½”ë“œì— ì¶”ê°€ í•„ë“œ ìƒì„± ë° ë°ì´í„° ë³€í™˜
4. EvaluateJsonPath: JSONì—ì„œ id í•„ë“œ ì¶”ì¶œ
5. PutElasticsearchHTTP: Elasticsearchì— ë°ì´í„° ì €ì¥

ê·¸ëŸ¬ë‚˜ ë§ˆì§€ë§‰ PutElasticsearchHTTP í”„ë¡œì„¸ì„œì—ì„œ ë‹¤ìŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:

```bash
Failed to process Flowfile due to failed to parse, transfering to failure
```

ë¡œê·¸ë¥¼ ìì„¸íˆ ì‚´í´ë³´ë‹ˆ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤:

```bash
Index operation upsert requires a valid identifier value from a flow file attribute, transferring to failure.
```

##### ë¬¸ì € ë¶„ì„

ë¡œê·¸ ë¶„ì„ ê²°ê³¼, ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:

1. ì›ì¸: PutElasticsearchHTTP í”„ë¡œì„¸ì„œê°€ 'upsert' ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì‹ë³„ì(ID) ê°’ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

2. ì„¸ë¶€ ì‚¬í•­:

    - FlowFile í¬ê¸°ê°€ ëª¨ë‘ 8ë°”ì´íŠ¸ë¡œ, ì‹¤ì œ JSON ë°ì´í„°ê°€ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ë†’ì•˜ìŠµë‹ˆë‹¤.
    - EvaluateJsonPath í”„ë¡œì„¸ì„œì—ì„œ idë¥¼ ì¶”ì¶œí•˜ê³  ìˆì—ˆì§€ë§Œ, ì´ ê°’ì´ PutElasticsearchHTTP í”„ë¡œì„¸ì„œì—ì„œ ì¸ì‹í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

##### í•´ê²° ë°©ë²•

1. PutElasticsearchHTTP í”„ë¡œì„¸ì„œ ì„¤ì • ìˆ˜ì •
PutElasticsearchHTTP í”„ë¡œì„¸ì„œì˜ ì„¤ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:

    - Identifier Attribute: id (EvaluateJsonPathì—ì„œ ì¶”ì¶œí•œ ì†ì„±ëª…ê³¼ ì¼ì¹˜í•˜ë„ë¡ ì„¤ì •)
    - Index Operation: ê¸°ì¡´ upsert ì„¤ì • í™•ì¸
    - Type: Elasticsearch ë²„ì „ì— ë§ê²Œ ì„¤ì • (7.x ì´ìƒì˜ ê²½ìš° ë¹„ì›Œë‘ê±°ë‚˜ _doc ì‚¬ìš©)

2. UpdateAttribute í”„ë¡œì„¸ì„œ ì¶”ê°€
EvaluateJsonPathì™€ PutElasticsearchHTTP ì‚¬ì´ì— UpdateAttribute í”„ë¡œì„¸ì„œë¥¼ ì¶”ê°€í•˜ì—¬ ID ê°’ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤:

    - ìƒˆ í”„ë¡œì„¸ì„œ: UpdateAttribute
    - ì†ì„± ì„¤ì •:

        - ì†ì„±ëª…: elasticsearch.id
        - ê°’: ${id} (EvaluateJsonPathì—ì„œ ì¶”ì¶œí•œ id ì†ì„± ì‚¬ìš©)

3. PutElasticsearchHTTP í”„ë¡œì„¸ì„œ ì„¤ì • ì¶”ê°€ ìˆ˜ì •
UpdateAttribute ì¶”ê°€ í›„, PutElasticsearchHTTP í”„ë¡œì„¸ì„œì˜ ì„¤ì •ì„ ë‹¤ì‹œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:

    - Identifier Attribute: elasticsearch.id (UpdateAttributeì—ì„œ ì„¤ì •í•œ ì†ì„±ëª…ìœ¼ë¡œ ë³€ê²½)

4. ë””ë²„ê¹…ì„ ìœ„í•œ LogAttribute í™œìš©
ë¬¸ì œ í•´ê²° ê³¼ì •ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ LogAttribute í”„ë¡œì„¸ì„œë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° íë¦„ì„ ëª¨ë‹ˆí„°ë§í–ˆìŠµë‹ˆë‹¤:

    - LogAttribute ì„¤ì •:

        - Log Level: INFO ë˜ëŠ” DEBUG
        - Log Payload: true (FlowFile ë‚´ìš©ì„ í•¨ê»˜ ë¡œê¹…)
        - Attributes to Log: all attributes (ëª¨ë“  ì†ì„± ë¡œê¹…)

##### ê²°ê³¼

<img src="./images/Figure 6.1.png" />

Elasticsearchì— ì •ìƒì ìœ¼ë¡œ ì €ì¥ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

```bash
app-py3.12 {seilylook} ğŸš€ curl -X GET "localhost:9200/_cat/indices?v"

health status index                            uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   .geoip_databases                 qhv1z7QnTAit-MJzq4nVxw   1   0         39            0     36.7mb         36.7mb
green  open   .kibana_task_manager_7.17.28_001 PkOLULcPR02nuAckO0GZQg   1   0         17          228      141kb          141kb
green  open   .apm-custom-link                 bCszQPeqTm6YGadNEOdDbw   1   0          0            0       227b           227b
green  open   .kibana_7.17.28_001              frGA6utWSFSA-o3uovgPBw   1   0         11            0      2.3mb          2.3mb
yellow open   scf                              L7BtsSUNQXK1FxOEmM_tPA   1   1       5000            0      1.7mb          1.7mb
green  open   .apm-agent-configuration         EKRPRn9aRbepTGm81kxlXQ   1   0          0            0       227b           227b
green  open   users                            hFnaau-_TtaDclMcC5Ocpw   1   0          0            0       227b           227b
```

<img src="./images/Figure 6.2.png" />

<img src="./images/Figure 6.3.png" />

<img src="./images/Figure 6.4.png" />

## Chapter 12. Building a Kafka Cluster

In stream processing, the data may be inifinite and incomplete at the time of a query. One of the leading tools in handling streaming data is **Apache Kafka**. Kafka is a tool that allows you to send dat in real time to topics. These topics can be read by consumers who process the data. 

### Creating zookeeper and Kafka clusters

```bash
services:
  # Zookeeper service
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
      - ./logs/zookeeper:/var/log/zookeeper
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - data-platform
  # Kafka Broker 1
  kafka1:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka1
    ports:
      - "9092:9092"
      - "29092:29092"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - kafka1-data:/var/lib/kafka/data
      - ./logs/kafka1:/var/log/kafka
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    networks:
      - data-platform

  # Kafka Broker 2
  kafka2:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka2
    ports:
      - "9093:9093"
      - "29093:29093"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093,PLAINTEXT_HOST://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - kafka2-data:/var/lib/kafka/data
      - ./logs/kafka2:/var/log/kafka
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9093", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    networks:
      - data-platform

  # Kafka Broker 3
  kafka3:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka3
    ports:
      - "9094:9094"
      - "29094:29094"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094,PLAINTEXT_HOST://localhost:29094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - kafka3-data:/var/lib/kafka/data
      - ./logs/kafka3:/var/log/kafka
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9094", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    networks:
      - data-platform

  # Kafka-UI
  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "8989:8080"
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    environment:
      KAFKA_CLUSTERS_0_NAME: data-platform-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092,kafka2:9093,kafka3:9094
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
    restart: always
    networks:
      - data-platform
```

#### Testing the kafka cluster

1. Topics í…ŒìŠ¤íŠ¸

```bash
docker exec -i -t kafka1 bash

kafka-topics --bootstrap-server kafka1:9092,kafka2:9093,kafka3:9094 --create --topic {TOPIC_NAME} --partitions 3 --replication-factor 3

Created topic dataengineering.
```

<img src="./images/Figure 12.1.png" />

2. Topics í™•ì¸

```bash
kafka-topics --bootstrap-server kafka1:9092 --list

dataengineering

kafka-topics --bootstrap-server kafka1:9092 --describe --topic dataengineering

Topic: dataengineering  TopicId: FhQDsGqqQVaJARfp--T_tw PartitionCount: 3       ReplicationFactor: 3    Configs: min.insync.replicas=2
        Topic: dataengineering  Partition: 0    Leader: 3       Replicas: 3,2,1 Isr: 3,2,1
        Topic: dataengineering  Partition: 1    Leader: 1       Replicas: 1,3,2 Isr: 1,3,2
        Topic: dataengineering  Partition: 2    Leader: 2       Replicas: 2,1,3 Isr: 2,1,3
```

3. Messages í…ŒìŠ¤íŠ¸

```bash
kafka-console-producer --bootstrap-server kafka1:9092 kafka2:9093 kafka3:9094 --topic dataengineering

> ì•ˆë…•í•˜ì„¸ìš” ë©”ì‹œì§€ 1ì…ë‹ˆë‹¤.
> ì•ˆë…•í•˜ì„¸ìš” ë©”ì‹œì§€ 2ì…ë‹ˆë‹¤.
> {"name": "í…ŒìŠ¤íŠ¸", "value": 123}
```

4. Read Message

ìƒˆë¡œìš´ í„°ë¯¸ë„ ì—´ê¸°

```bash
kafka-console-consumer --bootstrap-server kafka1:9092,kafka2:9093,kafka3:9094 --topic dataengineering --from-beginning

ì•ˆë…•í•˜ì„¸ìš” ë©”ì‹œï¿½ì§€ 1ì…ë‹ˆë‹¤.
ì•ˆë…•í•˜ì„¸ìš” ë©”ì‹œì§€ 2ì…ë‹ˆë‹¤.
{"ï¿½name": "í…ŒìŠ¤íŠ¸", "value": 123}

Processed a total of 4 messages
```

<img src="./images/Figure 12.2.png" />

### Producing and consuming with python

#### Writing a kafka producer in Python

```bash
app-py3.12 {seilylook} ğŸ’¡  î‚° ~/Development/Book/Data_Engineering_with_Python/app î‚° î‚  main Â± î‚° make start
=========================
Starting the application...
=========================
python -m src.main
2025-03-20 21:36:56,208 - root - INFO - ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: data/raw/test_data.csv
2025-03-20 21:36:56,208 - root - INFO - ========================
2025-03-20 21:36:56,208 - root - INFO - Kafka Topic & Message ìƒì„±
2025-03-20 21:36:56,208 - root - INFO - ========================
2025-03-20 21:36:56,227 - src.services.data_streaming - INFO - Kafka í´ëŸ¬ìŠ¤í„° ì—°ê²° ë° í† í”½ í™•ì¸ ì¤‘...
2025-03-20 21:36:56,255 - src.services.data_streaming - WARNING - 'users' í† í”½ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìë™ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2025-03-20 21:36:56,255 - src.services.data_streaming - INFO - 'data/raw/test_data.csv' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘
2025-03-20 21:36:56,261 - src.services.data_streaming - INFO - 100ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,264 - src.services.data_streaming - INFO - 200ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,267 - src.services.data_streaming - INFO - 300ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,270 - src.services.data_streaming - INFO - 400ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,272 - src.services.data_streaming - INFO - 500ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,275 - src.services.data_streaming - INFO - 600ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,277 - src.services.data_streaming - INFO - 700ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,280 - src.services.data_streaming - INFO - 800ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,282 - src.services.data_streaming - INFO - 900ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:56,285 - src.services.data_streaming - INFO - 1000ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘...
2025-03-20 21:36:58,524 - src.services.data_streaming - INFO - ì´ 1000ê°œ ë©”ì‹œì§€ê°€ 'users' í† í”½ìœ¼ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.
```

##### ë¬¸ì„¸ ìƒí™©

1. DNS í•´ì„ ì‹¤íŒ¨

```bash
%3|1742473184.651|FAIL|csv-producer#producer-1| [thrd:kafka2:9093/bootstrap]: kafka2:9093/bootstrap: Failed to resolve 'kafka2:9093': nodename nor servname provided, or not known (after 3ms in state CONNECT, 1 identical error(s) suppressed)
```

2. ì—°ê²° ì‹œê°„ ì´ˆê³¼

```bash
%4|1742473953.566|FAIL|csv-producer#producer-1| [thrd:172.18.0.7:9093/bootstrap]: 172.18.0.7:9093/bootstrap: Connection setup timed out in state CONNECT (after 30030ms in state CONNECT)
%4|1742473954.564|FAIL|csv-producer#producer-1| [thrd:172.18.0.6:9092/bootstrap]: 172.18.0.6:9092/bootstrap: Connection setup timed out in state CONNECT (after 30028ms in state CONNECT)
%4|1742473955.569|FAIL|csv-producer#producer-1| [thrd:172.18.0.8:9094/bootstrap]: 172.18.0.8:9094/bootstrap: Connection setup timed out in state CONNECT (after 30029ms in state CONNECT)
```

##### ë¬¸ì œ ì›ì¸

1. kafka ì´ì¤‘ ë¦¬ìŠ¤ë„ˆ ì„¤ì •

kafkaëŠ” Docker ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ë‘ ê°€ì§€ ë¦¬ìŠ¤ë„ˆë¥¼ ì‚¬ìš©í•œë‹¤.

- ë‚´ë¶€ í†µì‹ ìš© ë¦¬ìŠ¤ë„ˆ: `PLAINTEXT://kafka1:9092`

    - ì»¨í‹°ì—ë„ˆ ê°„ ë‚´ë¶€ í†µì‹ ì— ì‚¬ìš©ë¨

    - Docker ë‚´ë¶€ DNSë¡œ í•´ì„ë˜ì–´ì•¼ í•¨

- ì™¸ë¶€ ì ‘ê·¼ìš© ë¦¬ìŠ¤ë„ˆ: `PLAINTEXT_HOST://localhost:29092`

    - í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì—ì„œ ì ‘ê·¼í•  ë•Œ ì‚¬ìš©ë¨

    - ì™¸ë¶€ë¡œ í¬íŠ¸ê°€ ë…¸ì¶œë¨

2. êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ì›ì¸

- DNS í•´ì„ ì‹¤íŒ¨

    - í´ë¼ì´ì–¸íŠ¸ê°€ `kafka1:9092, kafka2:9093, kafka3:9094`ì™€ ê°™ì€ í˜¸ìŠ¤íŠ¸ëª…ì„ IPì£¼ì†Œë¡œ í•´ì„í•˜ì§€ ëª»í•¨

    - ì´ëŠ” Docker ë„¤íŠ¸ì›Œí¬ ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•˜ê±°ë‚˜, DNS ì„¤ì •ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒ.

- ì—°ê²° ì‹œê°„ ì´ˆê³¼

    - IP ì£¼ì†ŒëŠ” í•´ì„ë˜ì—ˆìœ¼ë‚˜ ì‹¤ì œ TCP ì—°ê²°ì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ

    - ì´ëŠ” ë³´í†µ ë°©í™”ë²½ ë¬¸ì œ, ë„¤íŠ¸ì›Œí¬ ë¶„ë¦¬, ë˜ëŠ” Kafka ì„¤ì • ë¬¸ì œë¡œ ë°œìƒ

##### í•´ê²° ê³¼ì •

1. ì ‘ê·¼ ë°©ì‹ ë³€ê²½: ë‚´ë¶€ í¬íŠ¸ì—ì„œ ì™¸ë¶€ í¬íŠ¸ë¡œ

kafka ë¸Œë¡œì»¤ì˜ ì™¸ë¶€ ë…¸ì¶œ í¬íŠ¸(29092, 29093, 29094)ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½

```python
# ë³€ê²½ ì „
bootstrap_servers = "kafka1:9092,kafka2:9093,kafka3:9094"

# ë³€ê²½ í›„
bootstrap_servers = "localhost:29092,localhost:29093,localhost:29094"
```

2. í•´ê²° ì›ë¦¬

- ë‚´ë¶€ í¬íŠ¸(9092, 9093, 9094):

    - Docker ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥

    - ì»¨í…Œì´ë„ˆ ê°„ ì§ì ‘ í†µì‹ ì— ì‚¬ìš©

- ì™¸ë¶€ í¬íŠ¸(29092, 29093, 29094):
    
    - í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì„ í†µí•´ ì ‘ê·¼

    - Docker ì»¨í…Œì´ë„ˆ ì™¸ë¶€ì—ì„œë„ ì ‘ê·¼ ê°€ëŠ¥

    - localhostë¡œ ë¼ìš°íŒ…ë¨

3. Docker composeì—ì„œì˜ kafka ì„¤ì • í™•ì¸

```yaml
KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092,PLAINTEXT_HOST://localhost:29092
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
```

- `ADVERTISED_LISTENERS`: Kafkaê°€ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ì—°ê²° ì •ë³´
- `LISTENER_SECURITY_PROTOCOL_MAP`: ê° ë¦¬ìŠ¤ë„ˆì˜ ë³´ì•ˆ í”„ë¡œí† ì½œ ì§€ì •
- `INTER_BROKER_LISTENER_NAME`: ë¸Œë¡œì»¤ ê°„ í†µì‹ ì— ì‚¬ìš©í•  ë¦¬ìŠ¤ë„ˆ ì§€ì •

##### ê²°ë¡ 

- ê°™ì€ Docker ë„¤íŠ¸ì›Œí¬ ë‚´: ì„œë¹„ìŠ¤ ì´ë¦„ê³¼ ë‚´ë¶€ í¬íŠ¸ (kafka1:9092)

- ì™¸ë¶€ ë˜ëŠ” ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬: localhostì™€ ì™¸ë¶€ í¬íŠ¸ (localhost:29092)


#### Writing a kafka consumer in Python

```bash
app-py3.12 âœ˜ {seilylook} ğŸ’¡  î‚° ~/Development/Book/Data_Engineering_with_Python/app î‚° î‚  main Â± î‚° make start
=========================
Starting the application...
=========================
python -m src.main
2025-03-20 23:02:45,286 - root - INFO - ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: data/raw/test_data.csv
2025-03-20 23:02:45,286 - root - INFO - ========================
2025-03-20 23:02:45,286 - root - INFO - Kafka Topic & Message ìƒì„±
2025-03-20 23:02:45,286 - root - INFO - ========================
2025-03-20 23:02:45,303 - src.services.data_streaming - INFO - í† í”½ êµ¬ë… ì‹œì‘: users
2025-03-20 23:02:45,303 - src.services.data_streaming - INFO - ë©”ì‹œì§€ ì†Œë¹„ ì‹œì‘...
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Kristina Parker', 'age': 68, 'street': '34674 Miller Overpass', 'city': 'Randallfurt', 'state': 'Maryland', 'zip': 40293, 'lng': 161.665903, 'lat': -87.125185}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Johnathan Lawson', 'age': 19, 'street': '95990 Williams Shore Apt. 829', 'city': 'Webbside', 'state': 'Maine', 'zip': 15543, 'lng': 146.494403, 'lat': -73.700935}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Rose Carpenter', 'age': 68, 'street': '444 Joseph Station', 'city': 'Pattersonside', 'state': 'New Mexico', 'zip': 79242, 'lng': 0.048327, 'lat': 74.385104}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Kimberly Santiago', 'age': 39, 'street': '7635 Peterson Spur Apt. 396', 'city': 'Tinaborough', 'state': 'Nevada', 'zip': 66267, 'lng': -38.278099, 'lat': -36.354147}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Wendy Murphy', 'age': 75, 'street': '35166 Ashlee Mills', 'city': 'Lawsonview', 'state': 'Massachusetts', 'zip': 30520, 'lng': -137.345477, 'lat': 35.262674}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Michael Lin', 'age': 18, 'street': '13086 Hall Pass', 'city': 'East Jay', 'state': 'New York', 'zip': 49686, 'lng': -52.411619, 'lat': -5.883704}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Wesley Watts', 'age': 61, 'street': '4541 Roth Brook Apt. 538', 'city': 'Hensleyland', 'state': 'Maine', 'zip': 70629, 'lng': 137.051209, 'lat': -35.1061065}
2025-03-20 23:02:49,507 - root - INFO - Received: {'name': 'Dennis Wolfe', 'age': 37, 'street': '474 Jones Plaza', 'city': 'Wardville', 'state': 'Minnesota', 'zip': 70795, 'lng': 19.632934, 'lat': -81.602252}
2025-03-20 23:02:49,508 - root - INFO - Received: {'name': 'Sharon Chandler', 'age': 21, 'street': '696 Michael Valleys Apt. 412', 'city': 'Lauraton', 'state': 'New Jersey', 'zip': 19419, 'lng': 14.510882, 'lat': 65.1203075}
2025-03-20 23:02:49,508 - root - INFO - Received: {'name': 'Amanda Mcmahon', 'age': 34, 'street': '96470 Cobb Hollow', 'city': 'Albertberg', 'state': 'Louisiana', 'zip': 22483, 'lng': -8.723311, 'lat': 27.196991}
2025-03-20 23:02:49,508 - root - INFO - Received: {'name': 'Peter Nguyen', 'age': 68, 'street': '15478 Dylan Crescent', 'city': 'North Katrinashire', 'state': 'New Jersey', 'zip': 96223, 'lng': 26.947073, 'lat': -9.097944}
2025-03-20 23:02:49,508 - root - INFO - Received: {'name': 'Matthew Robbins', 'age': 43, 'street': '4211 Brittany Field Suite 605', 'city': 'South Rebeccaborough', 'state': 'Delaware', 'zip': 19879, 'lng': 100.065663, 'lat': 54.933101}
2025-03-20 23:02:49,508 - root - INFO - Received: {'name': 'Michael Wilcox', 'age': 33, 'street': '018 Leon Alley', 'city': 'Johnmouth', 'state': 'New Mexico', 'zip': 73338, 'lng': -19.245506, 'lat': 26.5704125}
2025-03-20 23:02:49,508 - root - INFO - Received: {'name': 'Amanda Williams', 'age': 75, 'street': '44981 Rebecca Bypass', 'city': 'North Joseph', 'state': 'South Carolina', 'zip': 66529, 'lng': -24.771468, 'lat': 14.545032}
```